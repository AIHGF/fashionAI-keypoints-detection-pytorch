import numpy as np
import torch
import torch.nn.functional as F
from torch import nn


class CPNLoss(nn.Module):
    '''This is the loss function used for Cascaded Pyramid Net. Note that the
    original paper (arXiv:1711.07319) uses L2 loss. However the author (Shiyu)
    who participated in the FashionAI Keypoints competition found that L1 loss
    gave him a better result.

    Note that loss function is not used in test time. We simply want the
    predicted heatmaps generated by CPN.
    '''
    def __init__(self):
        super(CPNLoss, self).__init__()

    def l1_weighted_loss(self, hm_targets, hm_preds, vis_masks, ohkm=1.0):
        '''
        Args:
            hm_targets (torch.tensor): [batch_size, num_keypoints, h, w]
                Ground-truth heatmaps
            hm_preds (torch.tensor): [batch_size, num_keypoints, h, w]
                Predicted heatmaps
            vis_masks (torch.tensor): [batch_size, num_keypoints]
                Masks that indicate whether keypoints exist for each image.
            ohkm (float):
                Stands for 'Online Hard Keypoints Mining' rate. Closely
                related to 'Online Hard Example Mining (OHEM)'. Read:
                http://www.erogol.com/online-hard-example-mining-pytorch/

        Returns:
            float: A weighted loss between easy examples and hard examples.
        '''
        # Keeping only positive vales.
        # TODO:
        # 1. Not sure if this is necessary? I don't think we've introduced
        # any negative numbers?
        # 2. If this is necessary, shouldn't we use inplace=True?
        hm_preds = F.relu(hm_preds, inplace=False)

        bs, num_kpts, h, w = hm_targets.size()
        hm_targets = hm_targets.view(bs, num_kpts, -1)
        hm_preds = hm_preds.view(bs, num_kpts, -1)
        vis_masks = vis_masks.view(bs, num_kpts, 1).repeat(1, 1, h * w)

        # It picks the largest value across all the ground-truth heatmap
        # that generated by Gaussian function. Note that larger value indicates
        # the point has close proximity to ground-truth keypoint.
        # TODO: We could possibly skip this step by calculating the `amplitude`
        # in data generator.
        amplitude = torch.max(hm_targets)

        # We use `amplitude / 10` as a threshold to separate between
        # hard examples and easy examples.
        threshold = amplitude / 10

        # `easy_ids` are points that are close to ground-truth keypoints
        # (i.e. easier to predict) and `hard_ids` are points that are far away
        # from ground-truth keypoints (i.e. harder to predict).
        easy_ids = (((hm_targets > threshold) & (vis_masks >= 0))).float()
        hard_ids = (((hm_targets <= threshold) & (vis_masks >= 0))).float()

        # L1 loss between ground truth and predictions.
        diff = (hm_targets - hm_preds).abs()

        # We first sum across each pixels (i.e. `h * w`) with `sum(2)`,
        # then we sum across the batches (i.e. `batch_size`). We left with a
        # 1D-tensor with size `num_keypoints` for the category. Note that we
        # added epsilon in denomintor for numerical stability.
        # TODO: Do we really need to normalize the loss?
        epsilon = 0.0001
        easy_loss = (diff * easy_ids).sum(2).sum(0) / (easy_ids.sum(2).sum(0) + epsilon)
        hard_loss = (diff * hard_ids).sum(2).sum(0) / (hard_ids.sum(2).sum(0) + epsilon)

        total_loss = 0.5 * easy_loss + 0.5 * hard_loss

        # Find the top-k highest loss across all keypoints in each category.
        # Large loss = hard predictions = need to backprop.
        # Small loss = easy predictions = we just ignore it without backprop

        # Find the top-k keypoints that give us the highest loss.
        # High loss = hard predictions = Need to backprop to update NN weights.
        # Low loss = easy predictions = Ignore it without backprop.
        if ohkm < 1:
            k = int(total_loss.size(0) * ohkm)
            total_loss, _ = total_loss.topk(k)

        return total_loss.mean()

    def forward(self, hm_targets, hm_global_preds, hm_refine_preds, vis_masks):
        '''
        Args:
            hm_targets (torch.tensor): [batch_size, num_keypoints, h, w]
                Ground-truth heatmaps
            hm_global_preds (torch.tensor): [batch_size, num_keypoints, h, w]
                Predicted heatmaps (i.e. P2 layer) from GlobalNet.
            hm_refine_preds (torch.tensor): [batch_size, num_keypoints, h, w]
                Predicted heatmaps (i.e. concat output) from RefineNet.
            vis_masks (torch.tensor): [batch_size, num_keypoints]
                Masks that indicate whether keypoints exist for each image.
        Returns:
            float: Three different losses.
        '''
        global_loss = self.l1_weighted_loss(hm_targets, hm_global_preds, vis_masks)

        # We only use ohkm for the RefineNet as indicated in the original paper.
        refine_loss = self.l1_weighted_loss(hm_targets, hm_refine_preds, vis_masks, ohkm=0.5)

        # Only backprop the total loss. Individual losses are used for
        # metrics.
        return global_loss + refine_loss, global_loss, refine_loss

